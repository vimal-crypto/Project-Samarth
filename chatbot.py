
import os
import re
import json
import time
from typing import List, Dict, Any, Tuple

import numpy as np
import pandas as pd
import streamlit as st

# Try FAISS CPU
try:
    import faiss
except Exception:
    faiss = None

from sentence_transformers import SentenceTransformer

# -----------------------------
# Config
# -----------------------------
st.set_page_config(page_title="Project Samarth — Agri × Climate Chatbot", layout="wide")

CORPUS_PATH = "corpus.jsonl"            # generated by create_corpus.py
INDEX_PATH  = "samarth_index.faiss"     # FAISS index file
EMB_PATH    = "samarth_embeddings.npy"  # cached embeddings
MODEL_NAME  = "sentence-transformers/all-MiniLM-L6-v2"  # small & fast
TOP_K_DEFAULT = 8

# -----------------------------
# Helpers
# -----------------------------
def ensure_faiss():
    if faiss is None:
        st.error("FAISS not found. Install faiss-cpu and restart. See requirements.txt")
        st.stop()

@st.cache_data(show_spinner=False)
def load_corpus(corpus_path):
    if not os.path.exists(corpus_path):
        raise FileNotFoundError(
            "Missing corpus.jsonl. Run create_corpus.py once to build it from APY + IMD."
        )
    rows = []
    with open(corpus_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            obj = json.loads(line)
            rows.append({
                "text": obj.get("text", ""),
                "state": str(obj.get("meta", {}).get("state", "")).upper(),
                "crop":  str(obj.get("meta", {}).get("crop", "")).upper(),
                "year":  obj.get("meta", {}).get("year", None),
            })
    df = pd.DataFrame(rows)
    if df.empty or "text" not in df.columns:
        raise ValueError("corpus.jsonl is empty or malformed.")
    return df

@st.cache_resource(show_spinner=False)
def load_model(model_name):
    return SentenceTransformer(model_name)

def normalize_query(q):
    return (q or "").strip()

def summarize_years(years):
    years = [int(y) for y in years if str(y).isdigit()]
    if not years:
        return "N/A"
    years = sorted(set(years))
    return ("%d–%d" % (years[0], years[-1])) if len(years) > 1 else str(years[0])

# -----------------------------
# Index build/load
# -----------------------------
def embed_texts(model, texts, batch_size=256):
    embs = []
    n = len(texts)
    for i in range(0, n, batch_size):
        embs.append(model.encode(texts[i:i+batch_size],
                                 show_progress_bar=False,
                                 normalize_embeddings=True))
    return np.vstack(embs)

def build_or_load_index(df, model):
    """
    Returns (index, embeddings). Builds if files not present.
    """
    ensure_faiss()

    if os.path.exists(INDEX_PATH) and os.path.exists(EMB_PATH):
        try:
            index = faiss.read_index(INDEX_PATH)
            embs = np.load(EMB_PATH)
            if index.ntotal != embs.shape[0]:
                raise ValueError("Index and embedding count mismatch. Rebuild requested.")
            return index, embs
        except Exception:
            st.warning("Existing index invalid — rebuilding…")

    # Build new
    texts = df["text"].tolist()
    with st.spinner("Encoding corpus with MiniLM (one-time)…"):
        embs = embed_texts(model, texts)

    d = embs.shape[1]
    index = faiss.IndexFlatIP(d)  # cosine-sim (vectors are normalized)
    index.add(embs.astype(np.float32))

    faiss.write_index(index, INDEX_PATH)
    np.save(EMB_PATH, embs)
    return index, embs

# -----------------------------
# Retrieval + Answer composer
# -----------------------------
def search(index, model, query, df, top_k=8):
    q = normalize_query(query)
    if not q:
        return pd.DataFrame(columns=["score", "text", "state", "crop", "year"])
    q_emb = model.encode([q], normalize_embeddings=True)
    D, I = index.search(q_emb.astype(np.float32), top_k)
    scores = D[0].tolist()
    idxs   = I[0].tolist()

    hits = []
    for sc, ix in zip(scores, idxs):
        if ix < 0 or ix >= len(df):
            continue
        row = df.iloc[ix]
        hits.append({
            "score": float(sc),
            "text":  row["text"],
            "state": row.get("state", ""),
            "crop":  row.get("crop", ""),
            "year":  row.get("year", None),
        })
    return pd.DataFrame(hits)

def compose_answer(hits, user_q):
    """
    Narrative + top facts + source line.
    """
    if hits is None or hits.empty:
        return "I couldn’t find relevant facts for that query (within 1997–2017). Try specifying a state/crop/year."

    states = [s for s in hits["state"].dropna().astype(str).tolist() if s]
    crops  = [c for c in hits["crop"].dropna().astype(str).tolist() if c]
    years  = [y for y in hits["year"].dropna().tolist() if str(y).isdigit()]

    state_phrase = ", ".join(sorted(set(states))) if states else "the requested region"
    crop_phrase  = ", ".join(sorted(set(crops))) if crops else "the relevant crop(s)"
    year_phrase  = summarize_years(years)

    bullets = []
    for _, r in hits.head(6).iterrows():
        bullets.append("- " + str(r["text"]))

    text = (
        "**Summary:**\n\n"
        "Based on retrieved facts, here’s what the data suggests for **%s** and **%s** over **%s**:\n\n" %
        (state_phrase or "the region", crop_phrase or "crops", year_phrase)
        + "\n".join(bullets)
        + "\n\n**Interpretation:** These facts combine crop production/area/yield (APY) and annual rainfall (IMD). "
          "You can compare year by year using the values above.\n\n"
        "**Sources:** APY (1997–2017 slice), IMD (annual rainfall by state, mapped from sub-divisions)."
    )
    return text

# -----------------------------
# Simple Chart Parser (best-effort)
# -----------------------------
YEAR_RE  = re.compile(r"\b(19\d{2}|20\d{2})\b")
PROD_RE  = re.compile(r"produced\s+([0-9][0-9,\.]*)\s*tonnes", re.IGNORECASE)
RAIN_RE  = re.compile(r"(?:rainfall|annual rainfall)\s*:\s*([0-9][0-9,\.]*)\s*mm", re.IGNORECASE)

def extract_points_for_chart(hits):
    """
    Tries to parse (year, production_ton, rainfall_mm) from fact sentences.
    Returns a dataframe if >=3 points, else None.
    """
    pts = []
    for _, r in hits.iterrows():
        txt = r["text"]
        year = None
        prod = None
        rain = None

        y_m = YEAR_RE.search(txt)
        if y_m:
            year = int(y_m.group(1))

        p_m = PROD_RE.search(txt)
        if p_m:
            try:
                prod = float(str(p_m.group(1)).replace(",", ""))
            except Exception:
                prod = None

        r_m = RAIN_RE.search(txt)
        if r_m:
            try:
                rain = float(str(r_m.group(1)).replace(",", ""))
            except Exception:
                rain = None

        if year is not None and (prod is not None or rain is not None):
            pts.append({"year": year, "production_ton": prod, "rainfall_mm": rain})

    if not pts:
        return None
    df = pd.DataFrame(pts).drop_duplicates(subset=["year"]).sort_values("year")
    # require at least 3 points to draw a useful line chart
    if len(df) < 3:
        return None
    return df

# -----------------------------
# UI
# -----------------------------
st.title("Project Samarth — Agri × Climate Chatbot")
st.caption("Fully sourced answers — every fact is backed by government datasets (APY & IMD).")

# st.caption("RAG over APY (production) + IMD (rainfall). FAISS-CPU + MiniLM. No analytics engine.")

with st.sidebar:
    st.header("RAG Settings")
    top_k = st.slider("Top-K retrieved facts", min_value=4, max_value=20, value=TOP_K_DEFAULT, step=1)
    st.markdown("---")
    st.write("Index files:")
    st.code("%s\n%s\n%s" % (INDEX_PATH, EMB_PATH, CORPUS_PATH))
    st.markdown("---")
    if st.button("Rebuild Index"):
        if os.path.exists(INDEX_PATH): os.remove(INDEX_PATH)
        if os.path.exists(EMB_PATH):   os.remove(EMB_PATH)
        st.success("Cleared old index. It will rebuild on the next query.")

# Load corpus/model/index
try:
    corpus_df = load_corpus(CORPUS_PATH)
except Exception as e:
    st.error("Error loading corpus: %s" % e)
    st.stop()

model = load_model(MODEL_NAME)
index, _ = build_or_load_index(corpus_df, model)

# Overview (optional)
with st.expander("Dataset overview", expanded=False):
    n = len(corpus_df)
    uniq_states = sorted(corpus_df["state"].dropna().unique().tolist())
    uniq_crops  = sorted(corpus_df["crop"].dropna().unique().tolist())
    years = [y for y in corpus_df["year"].dropna().tolist() if str(y).isdigit()]
    st.write("**Corpus size:** %s facts" % f"{n:,}")
    st.write("**States (sample):** " + ", ".join(uniq_states[:15]))
    st.write("**Crops (sample):** " + ", ".join(uniq_crops[:15]))
    if years:
        st.write("**Year range:** %s–%s" % (min(years), max(years)))

# Chat state
if "chat" not in st.session_state:
    st.session_state.chat = []

# Render history
for i, m in enumerate(st.session_state.chat):
    with st.chat_message(m["role"]):
        st.write(m["content"])
        if m.get("table") is not None:
            st.dataframe(m["table"], use_container_width=True)
        # if m.get("chart") is not None:
        #     st.line_chart(m["chart"].set_index("year"))
        if m.get("download") is not None:
            st.download_button(
                "Download retrieved facts (.csv)",
                m["download"].to_csv(index=False).encode("utf-8"),
                file_name="retrieved_facts.csv",
                mime="text/csv",
                key="dl_%d_%d" % (i, int(time.time()*1000))  # unique key to avoid duplicate error
            )

# Input
user_q = st.chat_input("Ask about crops & rainfall, e.g. 'Rice vs rainfall in Karnataka 2005–2015'")
if user_q:
    # User message
    st.session_state.chat.append({"role": "user", "content": user_q})

    # Retrieve
    with st.spinner("Searching knowledge base…"):
        hits_df = search(index, model, user_q, corpus_df, top_k=top_k)

    # Compose final text
    answer = compose_answer(hits_df, user_q)

    # Try to derive a simple chart (optional)
    chart_df = extract_points_for_chart(hits_df)

    # Assistant message
    msg = {
        "role": "assistant",
        "content": answer,
        "table": hits_df[["score", "year", "state", "crop", "text"]],
        "download": hits_df[["score", "year", "state", "crop", "text"]],
    }
    if chart_df is not None:
        msg["chart"] = chart_df

    st.session_state.chat.append(msg)
    st.rerun()

st.markdown("---")
st.subheader("Citations")
st.write("""
- **APY (MoA&FW/DES)**: District-wise, season-wise crop production statistics (1997–2017 slice used here).
- **IMD (MoES)**: Sub-division rainfall (aggregated to annual, mapped to states).
- Note: Rainfall per state is approximated from sub-division → state mapping in corpus creation step.
""")
